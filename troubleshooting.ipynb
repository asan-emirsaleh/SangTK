{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# Running model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Loading model\n",
    "import pickle\n",
    "# Required for listing files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "# Loading/running model:\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the input robust to various 'boolean' inputs:\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# Converting the ab1 to the fasta:\n",
    "def abi_to_seq(input_ab1_file):\n",
    "    # Opening the abi file:\n",
    "    test_record = SeqIO.read(input_ab1_file, 'abi')\n",
    "    # Reading in the sequence:\n",
    "    letters = test_record.annotations['abif_raw']['PBAS1']\n",
    "    return letters\n",
    "###COMBINE THE FOLLOWING TWO FUNCTIONS??\n",
    "# Listing all ab1 files in directory\n",
    "def listing_ab1_files(input_dir):\n",
    "    # Getting all of the ab1 files:\n",
    "    onlyfiles = [f for f in listdir(input_dir) if\n",
    "                 isfile(join(input_dir, f)) if '.ab1' in f]\n",
    "    # Throwing on the directory to the front of the ab1 filenames:\n",
    "    outputlfiles = ['%s' % input_dir + each_file for each_file in onlyfiles]\n",
    "    return outputlfiles, onlyfiles\n",
    "\n",
    "# Listing all ab1 files in directory\n",
    "def listing_temp_files(input_dir):\n",
    "    # Getting all of the temp files:\n",
    "    onlyfiles = [f for f in listdir(input_dir) if\n",
    "                 isfile(join(input_dir, f)) if 'temp.ab1.conv.' in f]\n",
    "    # Throwing on the directory to the front of the ab1 filenames:\n",
    "    return onlyfiles\n",
    "\n",
    "# Converting sequence to fasta file:\n",
    "def seq_to_fa(input_name, input_seq, sequence_name = None):\n",
    "    # Getting the sequence name for the fasta:\n",
    "    if sequence_name == None:\n",
    "        sequence_name = input_name\n",
    "\n",
    "    # Generating the fasta file:\n",
    "    final_filename = input_name.rsplit('.', 1)[0] + '.fa'\n",
    "    final_file = open(final_filename, 'w')\n",
    "    final_file.write('> %s\\n' % sequence_name)\n",
    "    final_file.write(input_seq + '\\n')\n",
    "    final_file.close()\n",
    "    return final_file\n",
    "\n",
    "# Converting ab1 file to prediction input:\n",
    "def abi_to_df(input_seqio_record):\n",
    "    # Reading in the abi files:\n",
    "    input_seqio_record = SeqIO.read(input_seqio_record, 'abi')\n",
    "\n",
    "    # Getting the list of letters and their locations:\n",
    "    locations = list(input_seqio_record.annotations['abif_raw']['PLOC1'])\n",
    "    letters = list(input_seqio_record.annotations['abif_raw']['PBAS1'])\n",
    "\n",
    "    # Converting to df:\n",
    "    letter_loc_df = pd.DataFrame()\n",
    "    letter_loc_df['Locations'] = locations\n",
    "    letter_loc_df['Letters'] = letters\n",
    "\n",
    "    # Different df with all the waveform data:\n",
    "    peak_df = pd.DataFrame()\n",
    "    peak_df['g_let'] = list(input_seqio_record.annotations['abif_raw']['DATA9'])\n",
    "    peak_df['a_let'] = list(input_seqio_record.annotations['abif_raw']['DATA10'])\n",
    "    peak_df['t_let'] = list(input_seqio_record.annotations['abif_raw']['DATA11'])\n",
    "    peak_df['c_let'] = list(input_seqio_record.annotations['abif_raw']['DATA12'])\n",
    "\n",
    "    # Making the indeces play nicely and deleting the other column:\n",
    "    peak_df['index_plus_one'] = peak_df.index + 1\n",
    "    peak_df.index = peak_df['index_plus_one']\n",
    "    letter_loc_df.index = letter_loc_df['Locations']\n",
    "    letter_loc_df.drop('Locations', inplace=True, axis=1)\n",
    "\n",
    "    # combining the dfs:\n",
    "    combined_df = letter_loc_df.join(peak_df, how='inner')\n",
    "    return combined_df\n",
    "\n",
    "# Adding the previous and the following base to the df:\n",
    "def surrounding_bases(input_df):\n",
    "    previous_letter_value_df = input_df.shift(1)\n",
    "    previous_letter_value_df.dropna(inplace=True)\n",
    "    previous_letter_value_df.rename({'a_let':'prev_a','c_let':'prev_c','t_let':'prev_t','g_let':'prev_g'}, inplace=True, axis=1)\n",
    "\n",
    "    following_letter_value_df = input_df.shift(-1)\n",
    "    following_letter_value_df.dropna(inplace=True)\n",
    "    following_letter_value_df.rename({'a_let':'next_a','c_let':'next_c','t_let':'next_t','g_let':'next_g'}, inplace=True, axis=1)\n",
    "\n",
    "    current_previous_following_df = pd.concat([input_df, previous_letter_value_df, following_letter_value_df], axis=1, join='inner')\n",
    "    return current_previous_following_df\n",
    "\n",
    "def ab1_to_predicted_sequence_nonorm(input_ab1_file, model, actual_ab1=True, denormalize=False):\n",
    "    # Loading in and parsing input df:\n",
    "    if actual_ab1 == True:\n",
    "        test_df = abi_to_df(input_ab1_file)\n",
    "    else:\n",
    "        test_df = input_ab1_file\n",
    "    test_letter_value_df = test_df[['a_let', 'c_let', 't_let', 'g_let']]\n",
    "    # Rerun the nucleotide model on normalized values, but until then:\n",
    "    if denormalize == True:\n",
    "        test_letter_value_df = test_letter_value_df * 1000\n",
    "    test_full_info_df = surrounding_bases(test_letter_value_df)\n",
    "\n",
    "    # Using model to predict sequence:\n",
    "    predicted_probs_df = pd.DataFrame(model.predict(X=test_full_info_df),\n",
    "                                      columns=['Prediction'])\n",
    "\n",
    "    # Acquiring and returning sequence:\n",
    "    sequence = ''.join(list(predicted_probs_df['Prediction']))\n",
    "    return sequence\n",
    "\n",
    "# This combines a peak df with a full record\n",
    "def peak_calling_df(input_df, input_seqio_record):\n",
    "    input_df['peak_no_peak'] = [1] * input_df.shape[0]\n",
    "    input_df.index = input_df.index + 1####MAYBE KEEP THIS IN? MAYBE REMOVE IT?\n",
    "    first_val = input_df.index[0] - 5\n",
    "    last_val = input_df.index[-1] + 5\n",
    "    removed_df = input_df[['peak_no_peak']]\n",
    "    # Different df with all the waveform data:\n",
    "    peak_val = pd.DataFrame()\n",
    "    peak_val['g_let'] = list(input_seqio_record.annotations['abif_raw']['DATA9'])\n",
    "    peak_val['a_let'] = list(input_seqio_record.annotations['abif_raw']['DATA10'])\n",
    "    peak_val['t_let'] = list(input_seqio_record.annotations['abif_raw']['DATA11'])\n",
    "    peak_val['c_let'] = list(input_seqio_record.annotations['abif_raw']['DATA12'])\n",
    "\n",
    "    peak_val = peak_val.loc[first_val:last_val]\n",
    "    fin_df = removed_df.merge(peak_val, how='outer', left_index=True, right_index=True)\n",
    "    zero = fin_df[fin_df['peak_no_peak'] !=1]\n",
    "    zero['peak_no_peak'] = [0] * zero.shape[0]\n",
    "    nonzero = fin_df[fin_df['peak_no_peak'] ==1]\n",
    "    fin_df = zero.append(nonzero)\n",
    "    fin_df.sort_index(inplace=True)\n",
    "    return fin_df\n",
    "\n",
    "def slope(inp_df):\n",
    "    only_letters = inp_df[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    slope_before = only_letters.diff(1, axis=0)\n",
    "    slope_before.columns = ['slope_g_after', 'slope_a_after', 'slope_t_after', 'slope_c_after']\n",
    "    slope_after = only_letters.diff(-1, axis=0)\n",
    "    slope_after.columns = ['slope_g_before', 'slope_a_before', 'slope_t_before', 'slope_c_before']\n",
    "\n",
    "    final = only_letters.join(slope_before)\n",
    "    final = final.join(slope_after)\n",
    "    final = final.join(inp_df[['peak_no_peak']])\n",
    "    return final\n",
    "\n",
    "def normalizing(inp_df):\n",
    "    all_peak_places = inp_df[inp_df['peak_no_peak'] == 1]\n",
    "    all_peak_places_vals = all_peak_places[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    all_max_peaks = list(all_peak_places_vals.max(axis=1))\n",
    "    trimmed_mean = scipy.stats.trim_mean(all_max_peaks, proportiontocut=0.1)\n",
    "    inp_df = inp_df / trimmed_mean\n",
    "    inp_df['peak_no_peak'] = inp_df['peak_no_peak'] * trimmed_mean\n",
    "    inp_df['peak_no_peak'] = inp_df['peak_no_peak'].astype(int)\n",
    "    return inp_df\n",
    "\n",
    "def reshaping_the_df(inp_df, first_dim, second_dim, third_dim):\n",
    "    y_val_train = np.array(inp_df['peak_no_peak'])\n",
    "    inp_df = inp_df.iloc[:,:-1]\n",
    "    x_val_train = inp_df.values.reshape((first_dim,second_dim,third_dim))\n",
    "    return x_val_train, y_val_train\n",
    "\n",
    "\n",
    "# The function to swap out current peak for a better one\n",
    "def finding_taller_peak(the_input_df):\n",
    "    # Getting all of the peaks:\n",
    "    peaks = the_input_df[the_input_df['peak_no_peak'] == 1]\n",
    "    # Getting the indeces of all of the peaks:\n",
    "    peak_indeces = list(peaks.index)\n",
    "    # Getting the surrounding four rows:\n",
    "    before_peaks = [item - 4 for item in peak_indeces]\n",
    "    after_peaks = [item + 4 for item in peak_indeces]\n",
    "    # Going through each peak and respective rows:\n",
    "    for idx, item in enumerate(before_peaks):\n",
    "#         Making the smaller df:\n",
    "        temp_df = the_input_df.loc[item:after_peaks[idx]]\n",
    "        current_peak_loc = temp_df[temp_df['peak_no_peak'] == 1].index.tolist()[0]\n",
    "        just_letts = temp_df[['g_let','a_let','t_let','c_let']]\n",
    "#         Getting the max value of the dataframe:\n",
    "        max_val = max(just_letts.max(axis=1))\n",
    "#         Getting the index of the max value:\n",
    "        max_idx = int(temp_df[temp_df.values == max_val].index.tolist()[0])\n",
    "        max_letter = just_letts.loc[max_idx].idxmax(axis=1)\n",
    "\n",
    "#         Acquiring the slopes:\n",
    "        before_slopes = pd.DataFrame(temp_df.loc[max_idx][['slope_g_before','slope_a_before','slope_t_before','slope_c_before']]).T\n",
    "        before_slopes.columns = ['g_let','a_let','t_let','c_let']\n",
    "        after_slopes = pd.DataFrame(temp_df.loc[max_idx][['slope_g_after','slope_g_after','slope_g_after','slope_g_after']]).T\n",
    "        after_slopes.columns = ['g_let','a_let','t_let','c_let']\n",
    "\n",
    "#         If they're both positive:\n",
    "        val_1 = before_slopes[[max_letter]].values >= 0\n",
    "        val_1 = val_1[0][0]\n",
    "        val_2 = after_slopes[[max_letter]].values >= 0\n",
    "        val_2 = val_2[0][0]\n",
    "        if val_1 and val_2:\n",
    "            the_input_df['peak_no_peak'][current_peak_loc] = 0\n",
    "            the_input_df['peak_no_peak'][max_idx] = 1     \n",
    "\n",
    "    return the_input_df\n",
    "\n",
    "# Getting the called nucleotide from the cleaned up peak:\n",
    "def get_nuc_with_clean(input_index, input_df, df_or_val='val'):\n",
    "    if df_or_val == 'val':\n",
    "        og_nuc = input_df.iloc[input_df.index.get_loc(input_index,method='nearest')]['letter_vals']\n",
    "    else:\n",
    "        og_nuc = input_df.iloc[input_df.index.get_loc(input_index,method='nearest')]\n",
    "    return og_nuc\n",
    "\n",
    "def normalizing_nucleotides(inp_df):\n",
    "    all_peak_places_vals = inp_df[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    all_max_peaks = list(all_peak_places_vals.max(axis=1))\n",
    "    trimmed_mean = scipy.stats.trim_mean(all_max_peaks, proportiontocut=0.1)\n",
    "    all_peak_places_vals = all_peak_places_vals / trimmed_mean\n",
    "    return all_peak_places_vals\n",
    "\n",
    "\n",
    "def fixing_peaks_and_normalizing(input_ab1_file):\n",
    "    ab1_record = SeqIO.read(input_ab1_file, 'abi')\n",
    "    nucleotide_record = abi_to_df(input_ab1_file)\n",
    "    fin_training = peak_calling_df(nucleotide_record, ab1_record)\n",
    "    fin_training = normalizing(fin_training)\n",
    "    fin_training = slope(fin_training)\n",
    "    finding_taller_peak(fin_training)\n",
    "    final_df = fin_training[fin_training['peak_no_peak'] == 1]\n",
    "    final_df.reset_index(inplace=True)\n",
    "    final_df.drop('index', axis=1, inplace=True)\n",
    "    return final_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\r\n",
      "\u001b[34mab1_dir\u001b[m\u001b[m\r\n",
      "ab1_dir.fa\r\n",
      "\u001b[34mdata\u001b[m\u001b[m\r\n",
      "generating_and_evaluating_model.ipynb\r\n",
      "log_reg_default_million.sav\r\n",
      "log_reg_default_million_normed_fixed_peaks_tuned_parameters.sav\r\n",
      "model.h5\r\n",
      "sang.py\r\n",
      "troubleshooting.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = pickle.load(open('log_reg_default_million_normed_fixed_peaks_tuned_parameters.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = pickle.load(open('log_reg_default_million.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ab1_file = ('./ab1_dir/other/tsh_7_kb_9_17_19.custom3.ab1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/scipy/stats/stats.py:2825: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n",
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:200: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:230: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "first_round_df = fixing_peaks_and_normalizing(input_ab1_file)\n",
    "updated_predicted = ab1_to_predicted_sequence_nonorm(first_round_df, updated_model, actual_ab1=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ATAAGCACCAATTTATTATGCACACCTAGCTGAAAACCCAATTTTAGCTACAGCTTGGAGGCAAATAGCTGTCTTAGTCAAATAGCTATCAGTCCTTATCAATCATGAGCAGCCGACTGGTCAGAACACACTCATGTAAGCTCCACGTCTGGACAGCACCCATCAGACTGTTTCTGCGCCTTGCTCTGCTCTCTGTCCACAGATGCTCTGGCGTCTGGCGTCGCTGAGCTCCCCTTAGCTTTGTTTTCACAGGGCTGTCTGACTTGTGGATCTTTCTTCCTTCGAACGCACTCAAATGAGTGTCCTGCAAGGAATGCCAGCATCCATCCTGCAGAGAGAAGACATAGTCTCTCACTAACATACATAAAGAAGGTAAGAGTAAGTGAGTACAGACAAGATTGACTTTCACCACTACCAATAAATATAATTAATTTCACTCTCTATCAAATGCATGGCGTGCGATCTATCTAACACACACACAAATACCACCCATCCGACAAACAATGAATAAATAGACAAAGAATGGCTTGCATGATGGTTGCTGACAGAAATCATCAATGATGAACAAGAGTGACCTGACAGAAAACATACAAGGAATGAATGCCTGGGTGTGCGTTCAGATGGAGCGAACTAAATACTCCCGGACAGGAAGGAAGAAAGTCACTCGCTAGACAAACTGCATGTTGTCTGACTCCACCCGAAGGAGACAAAGAGTGTATCTTGCAAATAAAATCTAAAAGATAAGTGTTGGTTGGGCTAAGCGAATGCATAACAGTACAGATTACAAGTCATGGAAGGCAGGACCGAAACTTCCTTTTCTTTCCCATTTCCCATTAAATTTTAATTTTTGGAAATANATATTTTACNTAAAANCTCCAAGTTGCAAATGCCCAATAATTATAACCTTTACCCTATACCCATCCGCCCTGGCCCCCGCTTCCCCAAACCCATTTNCATTTTCTTTGGCCTTGGGCCATTTGCCCTTGCCACTGGGGGCNTGTGGTCAATATTTAGTTATTGACCAAAGAGGGATAGGGGGTCTTTCTCCTTTAGAAAGGGGAATAAGCCAGGAACCAATCCATACTGCTTTACCGTTTTGCCTTTTTGAAAATCTAACATAATACCTTAAAGGGGGCGTGGGCGGGGCACAAGGATTCAAAATTTGGGTAGGTTNCNAATCCCCAAAAACGGGATTTAGGGGGAATACCACAAAATCCCTTACCATGTGTGGAATGGGTGAGTTACCCTTAACCCTAACCNCCATTAGGCAGTGGGCCGCGGGGGTCTTGTTNNNNTTTCTTAGTCTNNANAAAAAAAGNGTAAAGGCGTGTGACACACTACCCCCNNCTNCTACTTTGATTTATTTGGGCACCCGAGGGCTATCATNGNGCCCTCTATCCTCTTCCCGAGATATAANGATANTN'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_predicted = ab1_to_predicted_sequence_nonorm(input_ab1_file, old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AGACGCACCAATTTATTATGCACACCTAGCTGAAAACCCAATTTTAGCTACAGTTTGGAGGCAAATAGCTGTCTTAGTCAAATAGCTATCAGTCCTTATCAATCATGAGCAGCCGACTGGTCAGAACACACTCATGTAAGCTCCACGTCTGGACAGCACCCATCAGACTGTTTCTGCGCCTTGCTCTGCTCTCTGTCCACAGATGCTCTGGCGTCTGGCGTCGCTGAGCTCCCCTTAGCTTTGTTTTCACAGGGCTGTCTGACTTGTGGATCTTTCTTCCTTCGAACGCACTCAAATGAGTGTGCTGCAAGGATTGCCAGCATCCATCATGCAGGGAGCATACTTAGTCTCTCACTAACATACATAATGAAGGTAAGAGTAAGTGAGTACAAACAGGATTGACAATAACCACCACCAATAACTATAATTTTACTCACTCTCTACCAAATGCATGGGGTGCGATCTATCTATCACTCACACAACTCCCACTCATCCAACAAACCTTGCATAATCAGACAAAGATTGGCTAGCGTGATGGTTGCTGACAGAAATCATGAATGATGAACAAGAGTGACCTGCCATAAAACATGCAAGGAATGACTGCCTGGGTGTGCGTTCAGATGGAGCAAACTAATCACTCCCGGACAGGAAGGAAGAAACTCACTCGCTAGACAAACTGCATGTTGTCTGACTCCACCCGAAGGAGACAAAGTGTGTATCTTGCAAATAAAATCTAGAAGATAAGTGTTGGTTGGGCTAANCGNATGCATAACAGTACAGANTACAANTCNTGGAANGCANGACCGAANCTTNCTTTTCTTTCCCATNTCCCATTNATTNTTANTATTTNGATNANNATATTTTACNTNNAANNNNCAANNTGNAAATGCCNAAAAATTTTAANCTTTCACCTCTNCCCATNNGCNCTGGCNCCNGCTTCCCCATACCCANTTNCTTTTTCTGTGGCCTTGGGCCATTNGCCNTTGCCACTGGGGGCNCGTAGNAAATNTTNAGNTATNGACCAAANAGNGATNNGNGNTCTTTCTCCTNTTGAANGGGNAATGAGNNANGANCNAATCNNTACNNNTTTNNCNNTNNGCCTTNNTNANANNNNANNANAATACNTNANAGGGNGNGTGGGNGGGGNACNNGGATNCNAAATTNNGNTATNTTNCNNATCCCNNAAAACGGGATNTAGGNGGAATACCNCNAAATCCNTTACCATNTGTNGANTNGGTGAGTTACCNTTAACCNTNACNNCNNTCAGNNANTGGGNCNCNNGNGTCTNNTNNNNNTNTCTTANTCTNNNNAAANNANGNGTATAGGNGNGNNNCACACTACNCCCNNNTNCTNCTTTGATATATNTNNGCNNNCNNGGGCTANCANTNNNCCATNTNNNCTCTTTNNAAGATATANNNATANTN'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ab1_dir/other/tsh_7_kb_9_17_19.custom3.ab1'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ab1_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the training df larger:\n",
    "fixed_path = './data/fixed_peaks/'\n",
    "unfixed_path = './data/unfixed_peaks/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly changing the names of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab1_file_2158.csv',\n",
       " 'ab1_file_4058.csv',\n",
       " 'ab1_file_3669.csv',\n",
       " 'ab1_file_2173.csv',\n",
       " 'ab1_file_1863.csv',\n",
       " 'ab1_file_605.csv',\n",
       " 'ab1_file_1527.csv',\n",
       " 'ab1_file_2348.csv',\n",
       " 'ab1_file_562.csv',\n",
       " 'ab1_file_1939.csv',\n",
       " 'ab1_file_3915.csv',\n",
       " 'ab1_file_4667.csv',\n",
       " 'ab1_file_3646.csv',\n",
       " 'ab1_file_2073.csv',\n",
       " 'ab1_file_2926.csv',\n",
       " 'ab1_file_2113.csv',\n",
       " 'ab1_file_1630.csv',\n",
       " 'ab1_file_2462.csv',\n",
       " 'ab1_file_1023.csv',\n",
       " 'ab1_file_2313.csv',\n",
       " 'ab1_file_2707.csv',\n",
       " 'ab1_file_1435.csv',\n",
       " 'ab1_file_3368.csv',\n",
       " 'ab1_file_1360.csv',\n",
       " 'ab1_file_2641.csv',\n",
       " 'ab1_file_1957.csv',\n",
       " 'ab1_file_1968.csv',\n",
       " 'ab1_file_1428.csv',\n",
       " 'ab1_file_2928.csv',\n",
       " 'ab1_file_4418.csv',\n",
       " 'ab1_file_286.csv',\n",
       " 'ab1_file_2081.csv',\n",
       " 'ab1_file_4285.csv',\n",
       " 'ab1_file_141.csv',\n",
       " 'ab1_file_3110.csv',\n",
       " 'ab1_file_2973.csv',\n",
       " 'ab1_file_3098.csv',\n",
       " 'ab1_file_3265.csv',\n",
       " 'ab1_file_3477.csv',\n",
       " 'ab1_file_4863.csv',\n",
       " 'ab1_file_2962.csv',\n",
       " 'ab1_file_2976.csv',\n",
       " 'ab1_file_4453.csv',\n",
       " 'ab1_file_1475.csv',\n",
       " 'ab1_file_4727.csv',\n",
       " 'ab1_file_4096.csv',\n",
       " 'ab1_file_3935.csv',\n",
       " 'ab1_file_4929.csv',\n",
       " 'ab1_file_4450.csv',\n",
       " 'ab1_file_4861.csv']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotide = './data/ab1_for_evaluation/'\n",
    "df_files = [f for f in listdir(nucleotide) if isfile(join(nucleotide, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_names = np.random.choice(len(df_files)*100,len(df_files), replace=False)\n",
    "for idx, item in enumerate(df_files):\n",
    "    new_name = 'ab1_file_nucleotide_%s.ab1' % str(random_names[idx])\n",
    "    # Renaming the fixed\n",
    "    os.rename('%s%s' % (nucleotide, item), '%s%s' % (nucleotide, new_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(df_files):\n",
    "    try:\n",
    "        new_name = 'ab1_file_%s.csv' % str(random_names[idx])\n",
    "        os.rename('%s%s' % (nucleotide, item), '%s%s' % (nucleotide, new_name))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3368, 2926, 4418, 2973, 3669, 1630, 2641, 4667, 3915, 2928, 2962,\n",
       "       2158, 3265, 4450, 1475, 3477,  141, 2113, 1023, 2707,  605, 1968,\n",
       "       1360, 1435, 1957, 3935, 4453, 2081, 2462, 1939, 2348, 2073, 4861,\n",
       "        562, 2173, 3646, 4058, 2976, 4863, 1527, 3098, 4285, 2313, 4727,\n",
       "       3110, 4096, 1863,  286, 4929, 1428])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
