{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# Running model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Loading model\n",
    "import pickle\n",
    "# Required for listing files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "# Loading/running model:\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the input robust to various 'boolean' inputs:\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "# Converting the ab1 to the fasta:\n",
    "def abi_to_seq(input_ab1_file):\n",
    "    # Opening the abi file:\n",
    "    test_record = SeqIO.read(input_ab1_file, 'abi')\n",
    "    # Reading in the sequence:\n",
    "    letters = test_record.annotations['abif_raw']['PBAS1']\n",
    "    return letters\n",
    "###COMBINE THE FOLLOWING TWO FUNCTIONS??\n",
    "# Listing all ab1 files in directory\n",
    "def listing_ab1_files(input_dir):\n",
    "    # Getting all of the ab1 files:\n",
    "    onlyfiles = [f for f in listdir(input_dir) if\n",
    "                 isfile(join(input_dir, f)) if '.ab1' in f]\n",
    "    # Throwing on the directory to the front of the ab1 filenames:\n",
    "    outputlfiles = ['%s' % input_dir + each_file for each_file in onlyfiles]\n",
    "    return outputlfiles, onlyfiles\n",
    "\n",
    "# Listing all ab1 files in directory\n",
    "def listing_temp_files(input_dir):\n",
    "    # Getting all of the temp files:\n",
    "    onlyfiles = [f for f in listdir(input_dir) if\n",
    "                 isfile(join(input_dir, f)) if 'temp.ab1.conv.' in f]\n",
    "    # Throwing on the directory to the front of the ab1 filenames:\n",
    "    return onlyfiles\n",
    "\n",
    "# Converting sequence to fasta file:\n",
    "def seq_to_fa(input_name, input_seq, sequence_name = None):\n",
    "    # Getting the sequence name for the fasta:\n",
    "    if sequence_name == None:\n",
    "        sequence_name = input_name\n",
    "\n",
    "    # Generating the fasta file:\n",
    "    final_filename = input_name.rsplit('.', 1)[0] + '.fa'\n",
    "    final_file = open(final_filename, 'w')\n",
    "    final_file.write('> %s\\n' % sequence_name)\n",
    "    final_file.write(input_seq + '\\n')\n",
    "    final_file.close()\n",
    "    return final_file\n",
    "\n",
    "# Converting ab1 file to prediction input:\n",
    "def abi_to_df(input_seqio_record):\n",
    "    # Reading in the abi files:\n",
    "    input_seqio_record = SeqIO.read(input_seqio_record, 'abi')\n",
    "\n",
    "    # Getting the list of letters and their locations:\n",
    "    locations = list(input_seqio_record.annotations['abif_raw']['PLOC1'])\n",
    "    letters = list(input_seqio_record.annotations['abif_raw']['PBAS1'])\n",
    "\n",
    "    # Converting to df:\n",
    "    letter_loc_df = pd.DataFrame()\n",
    "    letter_loc_df['Locations'] = locations\n",
    "    letter_loc_df['Letters'] = letters\n",
    "\n",
    "    # Different df with all the waveform data:\n",
    "    peak_df = pd.DataFrame()\n",
    "    peak_df['g_let'] = list(input_seqio_record.annotations['abif_raw']['DATA9'])\n",
    "    peak_df['a_let'] = list(input_seqio_record.annotations['abif_raw']['DATA10'])\n",
    "    peak_df['t_let'] = list(input_seqio_record.annotations['abif_raw']['DATA11'])\n",
    "    peak_df['c_let'] = list(input_seqio_record.annotations['abif_raw']['DATA12'])\n",
    "\n",
    "    # Making the indeces play nicely and deleting the other column:\n",
    "    peak_df['index_plus_one'] = peak_df.index + 1\n",
    "    peak_df.index = peak_df['index_plus_one']\n",
    "    letter_loc_df.index = letter_loc_df['Locations']\n",
    "    letter_loc_df.drop('Locations', inplace=True, axis=1)\n",
    "\n",
    "    # combining the dfs:\n",
    "    combined_df = letter_loc_df.join(peak_df, how='inner')\n",
    "    return combined_df\n",
    "\n",
    "# Adding the previous and the following base to the df:\n",
    "def surrounding_bases(input_df):\n",
    "    previous_letter_value_df = input_df.shift(1)\n",
    "    previous_letter_value_df.dropna(inplace=True)\n",
    "    previous_letter_value_df.rename({'a_let':'prev_a','c_let':'prev_c','t_let':'prev_t','g_let':'prev_g'}, inplace=True, axis=1)\n",
    "\n",
    "    following_letter_value_df = input_df.shift(-1)\n",
    "    following_letter_value_df.dropna(inplace=True)\n",
    "    following_letter_value_df.rename({'a_let':'next_a','c_let':'next_c','t_let':'next_t','g_let':'next_g'}, inplace=True, axis=1)\n",
    "\n",
    "    current_previous_following_df = pd.concat([input_df, previous_letter_value_df, following_letter_value_df], axis=1, join='inner')\n",
    "    return current_previous_following_df\n",
    "\n",
    "def ab1_to_predicted_sequence_nonorm(input_ab1_file, model, actual_ab1=True, denormalize=False):\n",
    "    # Loading in and parsing input df:\n",
    "    if actual_ab1 == True:\n",
    "        test_df = abi_to_df(input_ab1_file)\n",
    "    else:\n",
    "        test_df = input_ab1_file\n",
    "    test_letter_value_df = test_df[['a_let', 'c_let', 't_let', 'g_let']]\n",
    "    # Rerun the nucleotide model on normalized values, but until then:\n",
    "    if denormalize == True:\n",
    "        test_letter_value_df = test_letter_value_df * 1000\n",
    "    test_full_info_df = surrounding_bases(test_letter_value_df)\n",
    "\n",
    "    # Using model to predict sequence:\n",
    "    predicted_probs_df = pd.DataFrame(model.predict(X=test_full_info_df),\n",
    "                                      columns=['Prediction'])\n",
    "\n",
    "    # Acquiring and returning sequence:\n",
    "    sequence = ''.join(list(predicted_probs_df['Prediction']))\n",
    "    return sequence\n",
    "\n",
    "# This combines a peak df with a full record\n",
    "def peak_calling_df(input_df, input_seqio_record):\n",
    "    input_df['peak_no_peak'] = [1] * input_df.shape[0]\n",
    "    input_df.index = input_df.index + 1####MAYBE KEEP THIS IN? MAYBE REMOVE IT?\n",
    "    first_val = input_df.index[0] - 5\n",
    "    last_val = input_df.index[-1] + 5\n",
    "    removed_df = input_df[['peak_no_peak']]\n",
    "    # Different df with all the waveform data:\n",
    "    peak_val = pd.DataFrame()\n",
    "    peak_val['g_let'] = list(input_seqio_record.annotations['abif_raw']['DATA9'])\n",
    "    peak_val['a_let'] = list(input_seqio_record.annotations['abif_raw']['DATA10'])\n",
    "    peak_val['t_let'] = list(input_seqio_record.annotations['abif_raw']['DATA11'])\n",
    "    peak_val['c_let'] = list(input_seqio_record.annotations['abif_raw']['DATA12'])\n",
    "\n",
    "    peak_val = peak_val.loc[first_val:last_val]\n",
    "    fin_df = removed_df.merge(peak_val, how='outer', left_index=True, right_index=True)\n",
    "    zero = fin_df[fin_df['peak_no_peak'] !=1]\n",
    "    zero['peak_no_peak'] = [0] * zero.shape[0]\n",
    "    nonzero = fin_df[fin_df['peak_no_peak'] ==1]\n",
    "    fin_df = zero.append(nonzero)\n",
    "    fin_df.sort_index(inplace=True)\n",
    "    return fin_df\n",
    "\n",
    "def slope(inp_df):\n",
    "    only_letters = inp_df[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    slope_before = only_letters.diff(1, axis=0)\n",
    "    slope_before.columns = ['slope_g_after', 'slope_a_after', 'slope_t_after', 'slope_c_after']\n",
    "    slope_after = only_letters.diff(-1, axis=0)\n",
    "    slope_after.columns = ['slope_g_before', 'slope_a_before', 'slope_t_before', 'slope_c_before']\n",
    "\n",
    "    final = only_letters.join(slope_before)\n",
    "    final = final.join(slope_after)\n",
    "    final = final.join(inp_df[['peak_no_peak']])\n",
    "    return final\n",
    "\n",
    "def normalizing(inp_df):\n",
    "    all_peak_places = inp_df[inp_df['peak_no_peak'] == 1]\n",
    "    all_peak_places_vals = all_peak_places[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    all_max_peaks = list(all_peak_places_vals.max(axis=1))\n",
    "    trimmed_mean = scipy.stats.trim_mean(all_max_peaks, proportiontocut=0.1)\n",
    "    inp_df = inp_df / trimmed_mean\n",
    "    inp_df['peak_no_peak'] = inp_df['peak_no_peak'] * trimmed_mean\n",
    "    inp_df['peak_no_peak'] = inp_df['peak_no_peak'].astype(int)\n",
    "    return inp_df\n",
    "\n",
    "def reshaping_the_df(inp_df, first_dim, second_dim, third_dim):\n",
    "    y_val_train = np.array(inp_df['peak_no_peak'])\n",
    "    inp_df = inp_df.iloc[:,:-1]\n",
    "    x_val_train = inp_df.values.reshape((first_dim,second_dim,third_dim))\n",
    "    return x_val_train, y_val_train\n",
    "\n",
    "\n",
    "# The function to swap out current peak for a better one\n",
    "def finding_taller_peak(the_input_df):\n",
    "    # Getting all of the peaks:\n",
    "    peaks = the_input_df[the_input_df['peak_no_peak'] == 1]\n",
    "    # Getting the indeces of all of the peaks:\n",
    "    peak_indeces = list(peaks.index)\n",
    "    # Getting the surrounding four rows:\n",
    "    before_peaks = [item - 4 for item in peak_indeces]\n",
    "    after_peaks = [item + 4 for item in peak_indeces]\n",
    "    # Going through each peak and respective rows:\n",
    "    for idx, item in enumerate(before_peaks):\n",
    "#         Making the smaller df:\n",
    "        temp_df = the_input_df.loc[item:after_peaks[idx]]\n",
    "        current_peak_loc = temp_df[temp_df['peak_no_peak'] == 1].index.tolist()[0]\n",
    "        just_letts = temp_df[['g_let','a_let','t_let','c_let']]\n",
    "#         Getting the max value of the dataframe:\n",
    "        max_val = max(just_letts.max(axis=1))\n",
    "#         Getting the index of the max value:\n",
    "        max_idx = int(temp_df[temp_df.values == max_val].index.tolist()[0])\n",
    "        max_letter = just_letts.loc[max_idx].idxmax(axis=1)\n",
    "\n",
    "#         Acquiring the slopes:\n",
    "        before_slopes = pd.DataFrame(temp_df.loc[max_idx][['slope_g_before','slope_a_before','slope_t_before','slope_c_before']]).T\n",
    "        before_slopes.columns = ['g_let','a_let','t_let','c_let']\n",
    "        after_slopes = pd.DataFrame(temp_df.loc[max_idx][['slope_g_after','slope_g_after','slope_g_after','slope_g_after']]).T\n",
    "        after_slopes.columns = ['g_let','a_let','t_let','c_let']\n",
    "\n",
    "#         If they're both positive:\n",
    "        val_1 = before_slopes[[max_letter]].values >= 0\n",
    "        val_1 = val_1[0][0]\n",
    "        val_2 = after_slopes[[max_letter]].values >= 0\n",
    "        val_2 = val_2[0][0]\n",
    "        if val_1 and val_2:\n",
    "            the_input_df['peak_no_peak'][current_peak_loc] = 0\n",
    "            the_input_df['peak_no_peak'][max_idx] = 1     \n",
    "\n",
    "    return the_input_df\n",
    "\n",
    "# Getting the called nucleotide from the cleaned up peak:\n",
    "def get_nuc_with_clean(input_index, input_df, df_or_val='val'):\n",
    "    if df_or_val == 'val':\n",
    "        og_nuc = input_df.iloc[input_df.index.get_loc(input_index,method='nearest')]['letter_vals']\n",
    "    else:\n",
    "        og_nuc = input_df.iloc[input_df.index.get_loc(input_index,method='nearest')]\n",
    "    return og_nuc\n",
    "\n",
    "def normalizing_nucleotides(inp_df):\n",
    "    all_peak_places_vals = inp_df[['g_let', 'a_let', 't_let', 'c_let']]\n",
    "    all_max_peaks = list(all_peak_places_vals.max(axis=1))\n",
    "    trimmed_mean = scipy.stats.trim_mean(all_max_peaks, proportiontocut=0.1)\n",
    "    all_peak_places_vals = all_peak_places_vals / trimmed_mean\n",
    "    return all_peak_places_vals\n",
    "\n",
    "def fixing_peaks_and_normalizing(input_ab1_file, actual_ab1=True):\n",
    "    # Loading in and parsing input df:\n",
    "    if actual_ab1 == True:\n",
    "        nucleotide_df = abi_to_df(input_ab1_file)\n",
    "    else:\n",
    "        nucleotide_df = input_ab1_file\n",
    "#     nucleotide_letter_value_df = nucleotide_df[['a_let', 'c_let', 't_let', 'g_let']]\n",
    "    \n",
    "    # Reading in the ab1 file:\n",
    "    current_record = SeqIO.read(input_ab1_file, 'abi')\n",
    "    \n",
    "    # Generating the peak calling file\n",
    "    called_peak_df = peak_calling_df(nucleotide_df, current_record)\n",
    "    \n",
    "    # Cleaning up the called peak file\n",
    "    called_peak_df = normalizing(called_peak_df)\n",
    "    called_peak_df = slope(called_peak_df)\n",
    "    \n",
    "#     Commented this out, this is clearly not the problem...\n",
    "#     finding_taller_peak(called_peak_df)\n",
    "    \n",
    "    # Getting just the peaks:\n",
    "    clean_peaks = called_peak_df[called_peak_df['peak_no_peak'] == 1]\n",
    "    \n",
    "    # List of clean peak locs:\n",
    "    clean_peak_list = list(clean_peaks.index)\n",
    "\n",
    "    # Fixing the nucleotide's dataframe's index to get where the peak is:\n",
    "    nucleotide_df['letter_vals'] = nucleotide_df.index\n",
    "    nucleotide_df.index = nucleotide_df['index_plus_one']\n",
    "    nucleotide_df.drop('index_plus_one', axis=1, inplace=True)\n",
    "    \n",
    "    # List of called nucleotides from the clean list:\n",
    "    for each_peak_place, each_nucleotide_pos in enumerate(clean_peak_list):\n",
    "        if each_peak_place == 0:\n",
    "            final_peak_df = pd.DataFrame(get_nuc_with_clean(each_nucleotide_pos, \n",
    "                                                            nucleotide_df, df_or_val='df')).T\n",
    "        else:\n",
    "            final_peak_df = final_peak_df.append(pd.DataFrame(get_nuc_with_clean(each_nucleotide_pos, \n",
    "                                                                                 nucleotide_df,\n",
    "                                                                                 df_or_val='df')).T)\n",
    "        \n",
    "    final_peak_df = normalizing_nucleotides(final_peak_df)\n",
    "    \n",
    "    return final_peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\r\n",
      "\u001b[34mab1_dir\u001b[m\u001b[m\r\n",
      "ab1_dir.fa\r\n",
      "\u001b[34mdata\u001b[m\u001b[m\r\n",
      "generating_and_evaluating_model.ipynb\r\n",
      "log_reg_default_million.sav\r\n",
      "log_reg_default_million_normed_fixed_peaks_tuned_parameters.sav\r\n",
      "model.h5\r\n",
      "sang.py\r\n",
      "troubleshooting.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = pickle.load(open('log_reg_default_million_normed_fixed_peaks_tuned_parameters.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = pickle.load(open('log_reg_default_million.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ab1_file = ('./ab1_dir/gata2_31_200bpprom.custom2.ab1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/ipykernel_launcher.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/alexandredaly/anaconda/envs/python3/lib/python3.5/site-packages/scipy/stats/stats.py:2825: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "fixing_ab1_file_for_updated = fixing_peaks_and_normalizing(input_ab1_file)\n",
    "\n",
    "testtttt.reset_index(inplace=True)\n",
    "\n",
    "testtttt.drop('index', axis=1, inplace=True)\n",
    "\n",
    "updated_val = surrounding_bases(testtttt)\n",
    "predicted_probs_df = pd.DataFrame(updated_model.predict(X=updated_val),\n",
    "                                      columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_predicted = ''.join(list(predicted_probs_df['Prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTTTATTTAAGATGTTGCTAATAAGTTTCGGCCGCATCGGAACCTAGGCCAGTTCGTTACTCAAAATAAAAACGCAGAGAGATGTGAAGGTGGAAGGGAGGGAAGTGGAAGGGGTGTAGCTGGTGGAAGGAGGGTAAGAGGCAGTAGGACGTGGTAGCGCACGATACCAGAAAAAGGAGAGCGAGGTCTACCAGGAAGAGGCAGGCCTGCAGAGGAGAGAGGAGGGCAGGTGGAGGGGGGGCTTAAGTAGTGGGAGGAGTTTAAGGAGTAACGTTGAAACCTACGGCACTGTGACAGAAGGAGGCATATACTAACTCTGTAGCACCTTGCAAAGGGGCATCTACCTTTACGGGCCATCTGTCAATCAACCAGTGCACGGCCTGCATTGGGTTTATGTATATTCTGTATGCGGCATACTACCGGTGCGGTTTACGCTGCGTTGCTCTGGTCAGGGTTTGGCCCTCCTAGATATCAGCGGAAGCAGCCCTAGCAGGACGGCAACACCTATATGGTAAAGCCCGCAGGTAGCACGCTGCGAGTCCGCACAAACACACAAGTCTAGTTCTGGTAGTATATATACACCCAGCCAGTGTCCCACACTACCCTAACCGCGGGTCTATTGCAAGGTAACGCGGATGTCGCGGCACTGTATAAACAGTTGCTATGTGTAGTGGTTTTAGTAGTTAAGCTGTATGGTGCAAGCTGCACCCAAACCAGGTACCCGTTAGTGCTCGCTTTTGCGGTAAATGCGATATGGCTATTACGTGTGCTTTATATCGTATTTGTACCCGGGTAAGGGGTGTACGGCTGTTAGGTGTTTGTCTAAGTAGGCGGGGTGCCAAGTCTCTAGCGGCAAGTCAGCATTCAACCGGCCGGTGAAGGTAGGGCAGTAGGTTAGTAAAACGCAGCAGGTAGCACAAGGGGTTTAATCCTGTGTGTCTTGGCGCAGGGTGCTTTTTCCTTTAGCCCCTTACCAGGGGCACTTGACACAGCAGCCCCAGGGCAAAAAGAAATCCCCTCTTTGGTAGGGTTGCATAACCAGGACCAAAAAAAGTTGGGCTTTCGGTAAATTGTACCAAATTAAAACCCCCCTTTCGGTTAAAAAACCTTTCTTATTCCTCTTTGGTTAAAAGGGGCCTTTTTTCTTTTAGGCCCCTCCCCCCGCTTACCTTGCGGATTTAAAGGGGGAAAAAAAGTTAGGGGCCATGGTTCGGGGGCCCCGCCCCGTAAGGGGCCCCCAGTTTTTTAGGGGGTTCCCCGGTTTTTAACCCCCCTTCCTGGGAAAAAACCTTAAAGGGTTTTGGTTTAAAANTTGGTTCCAACCCCCCACATTTTAAAACCCTCCGGCCTTGGGGNANNGCCCCGGGGAGGGGGAAAAAAAGGGGGGGTTTTCCCCTGGGATGTTTTTGGAAACGGGGCAAAATGTCGTTCAAAAAGGGCAATTGCTTAAAAACCCCCCCCCCC'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_predicted = ab1_to_predicted_sequence_nonorm(input_ab1_file, old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTTTGTTTGGCGTCTTCATGGTGGCTTTACCAACAGTACCGGAATGCCAAGCTTACTTGATAGGGGTGGGGGACAGCGCGCGTCTCGGCCTCCGGCCCGCCCGGCTCCGGCCCCTCTGCATCCTCCGGCCGCCCTGGCGCCAGCTGCCGACTCCTGCACAGACGTGAAGCGGGGGCCGCGCACGCCTATGAAGCCGGCGCCAGCCAATCAGCGCCGCGCGCCGCCCAGCCTCCGCCCCCCCATTGGCTGCTCCCGCCGCTTTGGCCGCTGGACTTCGGGAATGACCAGATCTCGAGCGGCCGCCAGTGTGATGGATATCTGCAGAATTCAGGGCCCCAGTATGAATTTGACCCAAGTATCTAGGTAGGAAGCTCAGACCAATCAGTTCCCTTTGTCTGTGTTATCTGTCACCAGTGATGAACCTCACCTTTGACATCACTTCATATCCTAGCCCTTTCCAAATAATGCGTGTAGCACCGGCAGCAAATGCAGCCGACCAGGAGAATGTGTCCTGGGCAAACAGCCTGCAGACATCACGCTAACAGAGGGAGAGAGGCTATGCTTATCCTGCTGTGTGTGAGAAAGCAAGCTCTAAAGAGATGAAATGGAACACCCTATGTTCAGGCCTGGACACCGTCTACACCAGATCTGTGGGAGCTTCATGTCTCTGCTCCTTTTGCTGCTTGGCATCTGTCCTCAGGCATCAGAAAGGGAAGCCTGAAACTTGCTCATACATTTTCACCTGGGTCACGTGTCCATGTTGACTCTCATTTGTGTACTGTTTCTGAAACCCTGGCCCCTCTGACCATCTTGCCTCTTTCTATGGCTGCCACCCCTCAAGGCTATATGCACCAGGCTAGCAGTTAGGAACCAACCTCGGCCTGCCCAGCTGCCTTGCTGGGGACAGCAGCCTGCAGAGGCCCCTTTGGTAATCTCTCTATTCCACAGCCCTCATTTTTAATTTGCAAAATTGAAGCCCCAGATTCGAGAGCAGCAAAAGCCCAGGGGGCGGGTAAAATATTTCATGCCCTTCAGTGGAAGCCGAAGGGTGGGTTTCCAATTTACCTGGGTTCTGAAGGGTTGGGGAAAAAATTTACCTTGGGGGGAATTTATTGTTAATATTTCCTTGGGGCCCGAATTTTTTATTTTGCCAAAATAAAAAACATTGAATTCACCGTTTGGGCCCCGGGGGGGGCTTGCGCCAAGTCCTAACCCCAAAAAAAAAACTGGCCAAAAAAAGCTTTTTTGCCCCCTTAAAACTTTTTTGGAAAAAATTAATCCTGGGGGGAATTGGGCCCTTTTCCTTTGGGGNTTCCTTAAGGAAAAAAAAGTTTTTGGGAAAAAACCAATTCCACGGGNNAAAACCCCGCCCCCGGGGGGGCCCCCCTTTTTAAAAACCCGTCTTTTTCTGGGACCCCAAGGGTCTACTTAGGGGGCCCAGGTTCAAAAGGGTAAAAAAAAAAA'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
